{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wget\n",
    "import glob\n",
    "import zipfile\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from sklearn import preprocessing\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from shapely.geometry import Point"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "# Match Turnstile \"Unit\" with Station ID & Location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Download MTA station data with Station IDs and keep only Station Name, ID, and LAT LON\n",
    "station_ids = pd.read_csv(\"http://web.mta.info/developers/data/nyct/subway/Stations.csv\")\n",
    "station_ids = station_ids.drop(['Station ID', 'Complex ID', 'Borough', 'Line', 'Structure', 'Daytime Routes', 'Division'], axis=1)\n",
    "station_ids.columns = ['STOP_ID', 'STATION', 'LAT', 'LON']\n",
    "station_ids.STOP_ID = station_ids.STOP_ID.astype(str)\n",
    "\n",
    "\n",
    "#Download volunteer data matching Station IDs with Turnstile UNITS\n",
    "unit_ids = pd.read_csv(\"data/station_data/unit_ids.csv\")\n",
    "unit_ids = unit_ids.drop(['comments', 'previous errors', 'Line Name', 'Division', 'Station', 'Booth'], axis=1)\n",
    "unit_ids.columns = ['UNIT', 'STOP_ID']\n",
    "unit_ids.STOP_ID = unit_ids.STOP_ID.astype(str)\n",
    "unit_ids.UNIT = unit_ids.UNIT.astype(str)\n",
    "\n",
    "\n",
    "#Merge the LAT LON and NAMES with the UNITS on STATION IDs\n",
    "station_ids_with_unit_ids = station_ids.merge(unit_ids, on=\"STOP_ID\") \n",
    "station_ids_with_unit_ids = station_ids_with_unit_ids.drop(['STOP_ID'], axis=1)\n",
    "station_ids_with_unit_ids.to_csv(\"data/station_data/stations.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combine and Clean All Turnstile Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Open all downloaded text data, combine into one Dataframe, and clean\n",
    "path = \"data/text_data\"\n",
    "allFiles = glob.glob(path + \"/*.txt\")\n",
    "\n",
    "\n",
    "#Combine it all into one dataframe\n",
    "file_list = []\n",
    "for file in allFiles:\n",
    "    df = pd.read_csv(file,index_col=None,header=0)\n",
    "    file_list.append(df)  \n",
    "turnstile_data = pd.concat(file_list, axis = 0, ignore_index = True)\n",
    "\n",
    "\n",
    "#Remove extraneous data and remove non Subway lines like the PATH and Staten Island Railway\n",
    "turnstile_data.drop([\"C/A\", \"SCP\", \"DESC\", \"LINENAME\"], axis=1, inplace = True)\n",
    "turnstile_data.columns = ['UNIT', 'STATION', 'DIV', 'DATE', 'TIME', 'ENTRIES', 'EXITS']\n",
    "turnstile_data = turnstile_data[turnstile_data.DIV != \"SRT\"]\n",
    "turnstile_data = turnstile_data[turnstile_data.DIV != \"RIT\"]\n",
    "turnstile_data = turnstile_data[turnstile_data.DIV != \"PTH\"]\n",
    "turnstile_data.drop([\"DIV\"], axis=1, inplace = True)\n",
    "\n",
    "\n",
    "#Reformat the date and time creating a Pandas datetime\n",
    "turnstile_data['DATETIME'] = turnstile_data.DATE + ' ' + turnstile_data.TIME\n",
    "turnstile_data['DATETIME'] = pd.to_datetime(turnstile_data['DATETIME'], format='%m/%d/%Y %H:%M:%S')\n",
    "turnstile_data.drop([\"DATE\", \"TIME\", \"STATION\"], axis=1, inplace=True)\n",
    "\n",
    "\n",
    "#Because the exits and entries are cumulative, difference them\n",
    "turnstile_data.ENTRIES = turnstile_data.ENTRIES.diff()\n",
    "turnstile_data.EXITS = turnstile_data.EXITS.diff()\n",
    "\n",
    "\n",
    "#Remove negative numbers and extreme outliers\n",
    "turnstile_data = turnstile_data[turnstile_data.ENTRIES >= 0]\n",
    "turnstile_data = turnstile_data[turnstile_data.EXITS >= 0]\n",
    "turnstile_data = turnstile_data[turnstile_data.ENTRIES <= turnstile_data.ENTRIES.quantile(.99)]\n",
    "turnstile_data = turnstile_data[turnstile_data.EXITS <= turnstile_data.EXITS.quantile(0.99)]\n",
    "\n",
    "\n",
    "#Write out cleaned data to csv\n",
    "turnstile_data.to_csv(\"data/cleaned_data/nonaveraged_turnstile_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Group, Average, and Scale the Data based on Hour and Day of Week"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read in data, reformat datetime, and then groupby and aggregate by station and time\n",
    "turnstile_data = pd.read_csv(\"data/cleaned_data/nonaveraged_turnstile_data.csv\")\n",
    "turnstile_data['DATETIME'] = pd.to_datetime(turnstile_data['DATETIME'], format='%Y/%m/%d %H:%M:%S')\n",
    "turnstile_data = turnstile_data.groupby(['UNIT', 'DATETIME'], as_index=True).aggregate('sum')\n",
    "\n",
    "\n",
    "#Because the readings are every few hours, there are gaps in time. Moreover the timings don't match for each station. Thus we should interpolate and fill in the gaps.\n",
    "turnstile_data = turnstile_data.groupby(level=0).apply(lambda x: x.reset_index(level=0, drop=True).resample(\"1H\").interpolate(method='linear'))\n",
    "\n",
    "\n",
    "#Group and average the times so instead of days over years, we have an average for every hour of the day of week\n",
    "turnstile_data.reset_index(inplace=True)\n",
    "turnstile_data['HOUR'] = turnstile_data.DATETIME.dt.hour\n",
    "turnstile_data['DAYOFWEEK'] = turnstile_data.DATETIME.dt.dayofweek\n",
    "turnstile_data = turnstile_data.drop(['DATETIME'], axis=1)\n",
    "turnstile_data = turnstile_data.groupby(['UNIT', 'DAYOFWEEK', 'HOUR'], as_index=False).mean()\n",
    "\n",
    "\n",
    "#Normalize the Data across the Averaged Times as we don't want things like general population density to interfere\n",
    "entries = turnstile_data[['ENTRIES']].values.astype(float)\n",
    "entries_min_max_scaler = preprocessing.MinMaxScaler()\n",
    "entries_scaled = entries_min_max_scaler.fit_transform(entries)\n",
    "turnstile_data['NORMALIZED_ENTRIES'] = entries_scaled\n",
    "\n",
    "exits = turnstile_data[['EXITS']].values.astype(float)\n",
    "exits_min_max_scaler = preprocessing.MinMaxScaler()\n",
    "exits_scaled = exits_min_max_scaler.fit_transform(exits)\n",
    "turnstile_data['NORMALIZED_EXITS'] = exits_scaled\n",
    "\n",
    "turnstile_data['INTENSITY'] = turnstile_data['NORMALIZED_EXITS'] - turnstile_data['NORMALIZED_ENTRIES']\n",
    "turnstile_data = turnstile_data.drop(['NORMALIZED_ENTRIES', 'NORMALIZED_EXITS', 'ENTRIES', 'EXITS'], axis=1)\n",
    "\n",
    "\n",
    "#Merge the data with the Stations data to get geometry\n",
    "station_ids_with_unit_ids = pd.read_csv(\"data/station_data/stations.csv\")\n",
    "turnstile_data = turnstile_data.merge(station_ids_with_unit_ids, on=\"UNIT\")\n",
    "turnstile_data = turnstile_data.drop(['UNIT'], axis=1)\n",
    "turnstile_data = turnstile_data.drop_duplicates()\n",
    "\n",
    "\n",
    "#Write out the data to csv\n",
    "turnstile_data.to_csv(\"data/cleaned_data/averaged_turnstile_data.csv\", index=False)\n",
    "\n",
    "\n",
    "#Turn the lat and lon into coordinates\n",
    "turnstile_data['COORDINATES'] = list(zip(turnstile_data.LON.astype(float), turnstile_data.LAT.astype(float)))\n",
    "turnstile_data = turnstile_data.drop(['LON', 'LAT'], axis=1)\n",
    "\n",
    "\n",
    "#Turn coordinates into a geometry and write out a shapefile\n",
    "turnstile_data['COORDINATES'] = turnstile_data['COORDINATES'].apply(Point)\n",
    "turnstile_data = gpd.GeoDataFrame(turnstile_data, geometry='COORDINATES')\n",
    "turnstile_data.crs = {'init' :'epsg:4326'}\n",
    "turnstile_data.to_file(driver = 'ESRI Shapefile', filename= \"data/shapefiles/nonpivoted_averaged_turnstile_data.shp\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pivot the data so that each observation is a station and each column a specific time in the week"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read in the data\n",
    "turnstile_data = pd.read_csv(\"data/cleaned_data/averaged_turnstile_data.csv\")\n",
    "\n",
    "\n",
    "#Rename the days of the weeks and times for better readability\n",
    "daysofweek = {\n",
    "    0:'06SUN', \n",
    "    1:'00MON', \n",
    "    2:'01TUE', \n",
    "    3:'02WED', \n",
    "    4:'03THU', \n",
    "    5:'04FRI', \n",
    "    6:'05SAT'}\n",
    "\n",
    "times ={\n",
    "    '0':'00',\n",
    "    '1':'01',\n",
    "    '2':'02',\n",
    "    '3':'03',\n",
    "    '4':'04',\n",
    "    '5':'05',\n",
    "    '6':'06',\n",
    "    '7':'07',\n",
    "    '8':'08',\n",
    "    '9':'09'\n",
    "}\n",
    "turnstile_data.DAYOFWEEK = turnstile_data.DAYOFWEEK.replace(daysofweek)\n",
    "turnstile_data['HOUR'] = turnstile_data['HOUR'].astype(str).replace(times)\n",
    "turnstile_data['TIME'] = turnstile_data.DAYOFWEEK.astype('str') + turnstile_data.HOUR.astype('str') + \"00\"\n",
    "turnstile_data = turnstile_data.drop(['DAYOFWEEK', 'HOUR'], axis=1)\n",
    "\n",
    "\n",
    "#Pivot the data\n",
    "turnstile_data = pd.pivot_table(turnstile_data, values='INTENSITY', index='STATION', columns='TIME')\n",
    "\n",
    "\n",
    "#Merge pivoted data with station name and coordinates\n",
    "stations = pd.read_csv('data/station_data/stations.csv')\n",
    "stations = stations.drop(['UNIT'], axis=1)\n",
    "stations = stations.drop_duplicates('STATION')\n",
    "stations = stations.set_index('STATION')\n",
    "turnstile_data = pd.merge(turnstile_data.reset_index(), stations, on='STATION').set_index('STATION')\n",
    "turnstile_data = turnstile_data.reset_index()\n",
    "\n",
    "\n",
    "#Write out pivoted data to shapefile\n",
    "turnstile_data['COORDINATES'] = list(zip(turnstile_data.LON, turnstile_data.LAT))\n",
    "turnstile_data = turnstile_data.drop(['LON', 'LAT'], axis=1)\n",
    "turnstile_data['COORDINATES'] = turnstile_data['COORDINATES'].apply(Point)\n",
    "turnstile_data = gpd.GeoDataFrame(turnstile_data, geometry='COORDINATES')\n",
    "turnstile_data.crs = {'init' :'epsg:4326'}\n",
    "turnstile_data.to_file(driver = 'ESRI Shapefile', filename= \"data/shapefiles/pivoted_averaged_turnstile_data.shp\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (geospatial-analysis)",
   "language": "python",
   "name": "geospatial-analysis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "toc-autonumbering": false,
  "toc-showcode": true,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
